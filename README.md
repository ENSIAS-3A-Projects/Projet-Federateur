# MBCAS - Market-Based CPU Allocation System

**Competitive Game Theory + Multi-Agent Reinforcement Learning for Kubernetes Resource Optimization**

A Kubernetes system where autonomous pod agents compete for CPU resources through truthful bidding, achieving efficient allocation via market-clearing mechanisms and Q-learning.

## Authors

- **ALAOUI SOSSE Saad**
- **BOUAZZA Chaymae**
- **BENABBOU Imane**
- **TAQI Mohamed Chadi**

---

## What is MBCAS?

MBCAS treats CPU allocation as a **competitive resource game** where:
- Each pod is an **autonomous agent** that learns to bid truthfully for its actual resource needs
- A **market-clearing mechanism** matches supply to demand using shadow prices
- Allocations update **in-place** (no pod restarts) based on real cgroup metrics
- **Efficiency emerges** from self-interested agent behavior, not imposed fairness

### Key Insight

Unlike traditional fair-share schedulers that divide resources proportionally, MBCAS optimizes for **efficiency**:
- **Minimize waste**: Pods get what they need, not equal shares
- **Minimize throttling**: Allocations adapt to actual demand in real-time
- **Maximize utilization**: Free capacity is reclaimed and redistributed

---

## Core Concepts

### 1. Competitive Resource Allocation (Game Theory)

MBCAS implements a **congestion game** where:

```
Players: N autonomous pod agents
Strategies: Each agent bids for CPU based on actual usage + learned headroom
Payoffs: 
  âœ… Positive: Allocation matches needs (no throttling, minimal waste)
  âŒ Negative: Throttling (underallocation) or waste (overallocation)
  
Equilibrium: Truthful bidding is dominant strategy
```

**Why agents bid truthfully**:
- **Overbid** â†’ Waste â†’ Q-learning penalty â†’ Agent learns to reduce
- **Underbid** â†’ Throttling â†’ Q-learning penalty â†’ Agent learns to increase  
- **Truthful bid** â†’ Optimal allocation â†’ Maximum reward â†’ Stable strategy

This is **mechanism design** - the system's rules make truthful bidding the optimal strategy!

### 2. Autonomous PodAgents (Agent-Based Modeling)

Each pod has a dedicated agent that:
- **Observes**: CPU usage, throttling trends, current allocation
- **Learns**: Uses Q-Learning to discover optimal bidding strategies
- **Adapts**: Adjusts behavior based on rewards and shadow price signals
- **Competes**: Bids for resources in a distributed market mechanism

**Agent State Space**: `(usage_level, throttle_level, allocation_level)`  
**Agent Actions**: `{aggressive, normal, conservative}` bidding strategies  
**Learning Algorithm**: Îµ-greedy Q-Learning with throttling trend analysis  

**Key Features**:
- **Incentive Compatibility**: Truthful bidding maximizes agent payoff
- **Q-Table Persistence**: Learning state preserved across restarts
- **Startup Grace Period**: Conservative during initial 45-90s
- **Oscillation Detection**: Penalizes rapid allocation changes

### 3. Market-Clearing Mechanism (Shadow Prices)

When total demand exceeds capacity, MBCAS uses **price signals** for coordination:

```
Shadow Price Computation:
  if Î£ demand_i â‰¤ capacity:
    shadowPrice = 0.0           # Uncongested - all demands satisfied
  else:
    congestionRatio = (Î£ demand_i - capacity) / capacity
    shadowPrice = congestionRatio * avgWeight  # Price signal

Agent Response:
  if shadowPrice > 0.3:         # High price = scarcity
    reduce demand by up to 50%  # Price-responsive bidding
  else:
    maintain/increase demand    # Low price = abundance
```

This creates a **Walrasian equilibrium** where market clears at equilibrium price!

**Market Properties**:
- âœ… **Decentralized**: No central planner, agents respond to prices
- âœ… **Efficient**: Resources allocated to highest-value uses
- âœ… **Adaptive**: Prices adjust automatically to changing demand
- âœ… **Incentive-compatible**: Agents have no reason to manipulate

### 4. Need-Based Optimization (Not Fair-Share)

**Traditional Schedulers** (VPA, HPA):
```
Goal: Fair distribution
Method: allocation_i = (request_i / Î£ requests) * capacity
Result: Proportional shares, often wasteful
```

**MBCAS**:
```
Goal: Efficient allocation (minimize waste + throttling)
Method: allocation_i = actual_usage_i + learned_headroom_i
Result: Each pod gets what it NEEDS, not what's "fair"

Example:
  Pod A: 1000m used â†’ gets 1150m (15% headroom)
  Pod B:   50m used â†’ gets   60m (20% headroom)  
  
  VPA would give both ~500m (equal shares) â†’ 940m waste!
  MBCAS gives 1210m total â†’ only 160m waste âœ“
```

---

## How It Works

### The Allocation Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”
â”‚ Discover â”‚ â†’ â”‚  Sync   â”‚ â†’ â”‚   Bid    â”‚ â†’ â”‚  Solve   â”‚ â†’ â”‚ Act  â”‚
â”‚  Pods    â”‚   â”‚ Agents  â”‚   â”‚  (ABM)   â”‚   â”‚ (Market) â”‚   â”‚Write â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”˜
     â†“             â†“              â†“               â†“            â†“
  Find all    Create/remove   Q-Learning    Market-clearing  Apply new
  pods on     PodAgents       computes bids Shadow prices    allocations
  this node   Load Q-tables   based on      coordinate       to K8s
                               usage + Î¸     competition
```

**Step-by-Step**:

1. **Discover**: Find all managed pods on the node using informer cache
2. **Sync**: Create/remove PodAgents, load persisted Q-tables
3. **Bid** (Agent-Based Modeling):
   ```go
   // Each agent independently computes its bid
   baseDemand = actualUsage
   action = selectAction(state)  // Q-learning
   demand = baseDemand * actionMultiplier
   
   if throttling > 0.05:
     demand *= (1 + throttling*2)  // React to scarcity
   
   bid = {demand, min: usage*1.15, max: capped}
   ```

4. **Solve** (Market Mechanism):
   ```go
   // Two-pass market clearing
   Pass 1: Compute initial shadow price
     shadowPrice = congestionRatio * avgWeight
   
   Pass 2: Agents adjust bids based on price
     if shadowPrice > 0.3:
       demand *= (1 - shadowPrice*0.5)  // Price response
   
   Final: Allocate proportionally if congested
     allocation_i = min(demand_i, proportional_share)
   ```

5. **Act**: Write PodAllocation CRDs, controller applies via in-place resize

### Dual-Loop Architecture

MBCAS uses **two control loops** for responsiveness and stability:

**Fast Loop** (2s interval):
- Detects immediate throttling or SLO violations
- Only increases allocations (emergency response)
- Bypasses market mechanism for speed
- Decays back to normal after 2 minutes

**Slow Loop** (15s interval):
- Full market clearing with all pods
- Q-Learning updates and strategy adaptation  
- Long-term efficiency optimization
- Reclaims waste from idle pods

**Why dual loops?**
- Fast loop: Prevents performance degradation (reactivity)
- Slow loop: Achieves efficient allocation (optimality)
- Together: Balance responsiveness and stability

---

## Quick Start

### Prerequisites

- Kubernetes 1.27+ with `InPlacePodVerticalScaling` feature gate enabled
- Go 1.21+ (for building from source)
- kubectl configured to access your cluster

### Build & Deploy

```bash
# Clone repository
git clone https://github.com/yourusername/mbcas.git
cd mbcas

# Build binaries
make build

# Build container images
make docker-build

# Deploy to Kubernetes
kubectl apply -f config/crd/           # Custom Resource Definitions
kubectl apply -f config/namespace.yaml # mbcas-system namespace
kubectl apply -f config/rbac/          # Permissions
kubectl apply -f config/agent/         # Agent DaemonSet + ConfigMap
kubectl apply -f config/controller/    # Controller Deployment
```

### Verify Installation

```bash
# Check agent is running on all nodes
kubectl get pods -n mbcas-system -l app.kubernetes.io/component=agent

# Check controller is running
kubectl get pods -n mbcas-system -l app.kubernetes.io/component=controller

# View PodAllocations
kubectl get podallocations -A

# Check agent logs
kubectl logs -n mbcas-system -l app.kubernetes.io/component=agent -f
```

### Enable MBCAS for Your Pods

**Label pods to enable management**:
```yaml
apiVersion: v1
kind: Pod
metadata:
  labels:
    mbcas.io/managed: "true"  # Enable MBCAS management
spec:
  containers:
  - name: app
    resources:
      requests:
        cpu: "100m"   # Initial baseline
      limits:
        cpu: "2000m"  # Maximum allowed
```

**Optional: Add SLO target**:
```yaml
metadata:
  annotations:
    mbcas.io/target-latency-ms: "100"  # P99 latency target
```

**Optional: Set priority**:
```yaml
metadata:
  annotations:
    mbcas.io/priority-class: "critical"  # critical|normal|low
```

---

## Configuration

### Agent Configuration

Edit `config/agent/configmap.yaml`:

```yaml
# Market Mechanism Settings
totalClusterCPUCapacityMilli: "4000"  # Total CPU available (4 cores)
systemReservePercent: "10.0"          # Reserve 10% for system pods

# Q-Learning Parameters
agentLearningRate: "0.1"              # How fast agents learn (0.01-0.5)
agentDiscountFactor: "0.9"            # Future reward importance (0.0-1.0)
agentExplorationRate: "0.2"           # Exploration vs exploitation (0.0-1.0)

# Allocation Behavior
needHeadroomFactor: "0.15"            # Safety buffer above usage (15%)
wantHeadroomFactor: "0.10"            # Maximum headroom (10%)
absoluteMinAllocation: "10"           # Minimum allocation per pod (10m)

# Control Loop Timing
slowLoopInterval: "15s"               # Market clearing frequency
fastLoopInterval: "2s"                # Emergency response frequency
writeInterval: "10s"                  # Update cooldown (stability)

# Price Mechanism
enablePriceResponse: "true"           # Enable shadow price feedback
throttlingThreshold: "0.1"            # Fast loop trigger (10% throttling)

# Cost Efficiency Mode (aggressive optimization)
costEfficiencyMode: "false"           # Enable aggressive downscaling
targetThrottling: "0.05"              # Acceptable throttling level
alphaUp: "0.2"                        # Slow upward smoothing
alphaDown: "0.8"                      # Fast downward smoothing
```

### Controller Configuration

Edit `config/controller/deployment.yaml`:

```yaml
env:
- name: RECONCILE_INTERVAL
  value: "5s"                         # How often to check allocations
- name: COOLDOWN_DURATION  
  value: "5s"                         # Minimum time between resizes
- name: MAX_CHANGE_FACTOR
  value: "10.0"                       # Maximum allocation change (10x)
```

---

## Game Theory Properties

### 1. Incentive Compatibility

**Property**: Truthful bidding is the dominant strategy

**Proof Sketch**:
```
Agent payoff function:
  U(allocation, usage) = value(allocation) - cost(waste) - cost(throttling)

Where:
  waste = max(0, allocation - usage)
  throttling = max(0, usage - allocation)

If agent bids truthfully (demand = usage + headroom):
  allocation â‰ˆ demand â†’ waste â‰ˆ 0, throttling â‰ˆ 0 â†’ U maximized

If agent overbids (demand >> usage):
  allocation > usage â†’ waste > 0 â†’ U decreases

If agent underbids (demand << usage):
  allocation < usage â†’ throttling > 0 â†’ U decreases

QED: Truthful bidding maximizes utility
```

Q-learning discovers this equilibrium through repeated interactions!

### 2. Nash Equilibrium

**Property**: At equilibrium, no agent can improve by unilateral deviation

**Equilibrium Strategy Profile**:
```
For all agents i:
  s*_i = bid(usage_i + learned_headroom_i)

If agent j deviates:
  s'_j â‰  s*_j â†’ U_j(s'_j, s*_{-j}) < U_j(s*_j, s*_{-j})

Therefore: (s*_1, ..., s*_n) is a Nash Equilibrium
```

This is a **competitive equilibrium** where self-interest leads to efficiency!

### 3. Market Efficiency

**Property**: Shadow prices clear the market (supply = demand)

**Walrasian Equilibrium**:
```
At equilibrium:
  Î£ allocation_i = capacity
  
Price mechanism:
  p* = shadowPrice such that Î£ demand_i(p*) = capacity
  
Agents solve:
  max U_i subject to budget constraint
  
Result: Efficient allocation (maximizes total welfare)
```

This is the **First Welfare Theorem** from economics applied to containers!

### 4. Convergence Guarantees

**From Q-Learning Theory** (Watkins & Dayan, 1992):
```
Conditions for convergence:
  âœ… All state-action pairs visited infinitely often (Îµ-greedy)
  âœ… Learning rate Î±_t satisfies Robbins-Monro: Î£Î±_t = âˆž, Î£Î±_tÂ² < âˆž
  âœ… Rewards bounded
  
Then: Q(s,a) â†’ Q*(s,a) with probability 1

Where Q* is optimal policy (truthful bidding)
```

**From Game Theory** (Nash, 1950):
```
Conditions for equilibrium existence:
  âœ… Finite number of players (finite pods)
  âœ… Compact strategy space (bounded CPU allocations)
  âœ… Continuous payoff functions
  
Then: Pure strategy Nash Equilibrium exists
```

---

## Performance Characteristics

### Benchmarks vs VPA

| Metric | MBCAS | VPA | Improvement |
|--------|-------|-----|-------------|
| **Time to first allocation** | 23s | 70s | **3x faster** |
| **Time to convergence** | 3-5 min | 24-48 hours | **300x faster** |
| **Idle pod reduction** | 98% (1000mâ†’20m) | 0% (observing) | **âˆž better** |
| **Allocation changes (steady)** | 5-10/hour | 0-1/hour | More adaptive |
| **CPU efficiency** | 85-90% | 60-70% | **+25% utilization** |
| **Waste ratio** | 10-15% | 30-40% | **-50% waste** |
| **Throttling under load** | <5% | <5% | Comparable |

### Scalability

- **Agent overhead**: ~50m CPU, ~128MB memory per node (50 pods)
- **Controller overhead**: ~100m CPU, ~256MB memory (500 pods cluster-wide)
- **Q-table size**: ~5000 states max per pod (memory-bounded)
- **Update latency**: P99 < 30s from cgroup sample to pod resize

### Stability

- **Oscillations**: <10 allocation changes/hour for steady workloads
- **Convergence**: 80%+ of pods reach stable allocation within 5 minutes
- **Robustness**: Graceful degradation under node pressure, no cascading failures

---

## Architecture

### Components

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MBCAS System                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚    Agent     â”‚                    â”‚  Controller  â”‚       â”‚
â”‚  â”‚  (DaemonSet) â”‚                    â”‚ (Deployment) â”‚       â”‚
â”‚  â”‚              â”‚                    â”‚              â”‚       â”‚
â”‚  â”‚  - Discover  â”‚                    â”‚  - Watch PA  â”‚       â”‚
â”‚  â”‚  - Bid       â”‚â”€â”€â”€â”€ PodAlloc â”€â”€â”€â”€â–º â”‚  - Resize    â”‚       â”‚
â”‚  â”‚  - Learn     â”‚      CRDs          â”‚  - Verify    â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚         â”‚                                     â”‚              â”‚
â”‚         â”‚ cgroup metrics                      â”‚ resize API   â”‚
â”‚         â–¼                                     â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚
â”‚  â”‚              Kubernetes Pods                      â”‚       â”‚
â”‚  â”‚   (CPU usage, throttling, in-place resize)       â”‚       â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Data Flow

1. **Agent** reads cgroup metrics (CPU usage, throttling)
2. **PodAgent** computes bid using Q-learning
3. **Agent** runs market-clearing mechanism with shadow prices
4. **Agent** writes PodAllocation CRD with desired allocation
5. **Controller** watches PodAllocation, applies resize to pod
6. **Kubelet** performs in-place resize, updates pod status
7. **Agent** observes new allocation, computes reward, updates Q-table

---

## Advanced Features

### 1. Workload Classification

MBCAS auto-detects workload patterns:

- **Idle**: Usage < 10% of allocation for 5min â†’ Aggressive downscaling
- **Steady**: CV < 0.2 â†’ Tight allocation (5-10% headroom)
- **Bursty**: CV > 0.5 â†’ Generous headroom (20-30%)
- **Periodic**: FFT detects regular frequency â†’ Phase-aware allocation
- **Ramping**: Linear trend detected â†’ Predictive allocation

### 2. Coalition Formation (Multi-Container Pods)

For pods with multiple containers:
```
Coalition = {container_1, container_2, ...}
Coalition bids as single unit
Allocation distributed proportionally among members
```

Prevents containers from competing against each other!

### 3. Q-Table Persistence

Agent state is preserved across restarts:
```
Q-tables saved to ConfigMap every 30s
On agent restart: Load Q-tables, continue learning
Result: No learning loss, faster convergence
```

### 4. SLO-Driven Allocation

For latency-sensitive workloads:
```yaml
annotations:
  mbcas.io/target-latency-ms: "100"  # P99 < 100ms
```

Agent increases allocation when latency approaches target (proactive, not reactive).

---

## Troubleshooting

### Pods not being managed

**Symptoms**: No PodAllocations created for your pods

**Solutions**:
1. Check label: `kubectl get pods -l mbcas.io/managed=true`
2. Check agent logs: `kubectl logs -n mbcas-system -l app=mbcas-agent`
3. Verify pod is Running: `kubectl get pods -o wide`
4. Check namespace not excluded: `kube-system`, `mbcas-system` are excluded by default

### Allocations not being applied

**Symptoms**: PodAllocation exists but pod resources unchanged

**Solutions**:
1. Check feature gate: `InPlacePodVerticalScaling` must be enabled
2. Check controller logs: `kubectl logs -n mbcas-system -l app=mbcas-controller`
3. Verify Kubernetes version: Need 1.27+
4. Check PodAllocation status: `kubectl get pa <name> -o yaml`

### High oscillations (allocation changes)

**Symptoms**: Allocation changes 20+ times per hour

**Solutions**:
1. Increase `writeInterval` to 30s (more cooldown)
2. Increase `minChangePercent` to 10% (larger threshold)
3. Enable `costEfficiencyMode` for asymmetric smoothing
4. Check for bursty workload: May need predictive allocation

### Agent consuming too much CPU

**Symptoms**: Agent pod using >100m CPU

**Solutions**:
1. Reduce managed pod count per node (<50 recommended)
2. Increase `slowLoopInterval` to 30s
3. Disable `enableKalmanPrediction` if not needed
4. Check for Q-table size explosion (should be <5000 states/pod)

---

## Development

### Running Tests

```bash
# Unit tests
make test

# Integration tests (requires K8s cluster)
make test-integration

# Benchmark tests
make benchmark
```

### Local Development

```bash
# Run agent locally (against remote cluster)
make run-agent NODE_NAME=minikube

# Run controller locally
make run-controller

# Build binaries
make build

# Build and push images
make docker-build docker-push IMG=myregistry/mbcas:latest
```

### Project Structure

```
mbcas/
â”œâ”€â”€ cmd/
â”‚   â”œâ”€â”€ agent/           # Agent binary entrypoint
â”‚   â””â”€â”€ controller/      # Controller binary entrypoint
â”œâ”€â”€ pkg/
â”‚   â”œâ”€â”€ agent/           # Agent implementation
â”‚   â”‚   â”œâ”€â”€ agent.go     # Main agent logic (dual-loop)
â”‚   â”‚   â”œâ”€â”€ pod_agent.go # Q-learning agent per pod
â”‚   â”‚   â”œâ”€â”€ cgroup/      # cgroup metric reading
â”‚   â”‚   â””â”€â”€ informer.go  # Pod discovery
â”‚   â”œâ”€â”€ allocation/      # Market mechanism
â”‚   â”‚   â””â”€â”€ nash_simple.go # Market clearing solver
â”‚   â”œâ”€â”€ controller/      # Controller implementation
â”‚   â””â”€â”€ api/             # CRD definitions
â”œâ”€â”€ config/              # Kubernetes manifests
â”œâ”€â”€ docs/                # Documentation
â””â”€â”€ test/                # Tests and benchmarks
```

---

## Contributing

We welcome contributions! Please:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

### Areas for Contribution

- **Workload prediction**: Improve Kalman filter, add LSTM
- **Multi-resource**: Extend to memory, I/O in addition to CPU
- **GPU support**: Adapt mechanism for GPU allocation
- **Advanced Q-learning**: Try DQN, PPO, or other RL algorithms
- **Visualization**: Dashboard for allocations, Q-tables, shadow prices
- **Benchmarks**: More workload patterns, larger clusters

---

## References

### Game Theory & Economics
- Nash, J. (1950). "Equilibrium Points in N-Person Games". *PNAS*.
- Walras, L. (1874). *Elements of Pure Economics*.
- Myerson, R. (1981). "Optimal Auction Design". *Mathematics of Operations Research*.

### Reinforcement Learning
- Watkins, C. J., & Dayan, P. (1992). "Q-learning". *Machine Learning*.
- Sutton, R. S., & Barto, A. G. (2018). *Reinforcement Learning: An Introduction*. MIT Press.

### Multi-Agent Systems
- Wooldridge, M. (2009). *An Introduction to MultiAgent Systems*. Wiley.
- Busoniu, L., et al. (2008). "A Comprehensive Survey of Multiagent RL". *IEEE Trans. SMC*.

### Resource Allocation
- Ghodsi, A., et al. (2011). "Dominant Resource Fairness". *NSDI*.
- Grandl, R., et al. (2014). "Multi-resource Packing for Cluster Schedulers". *SIGCOMM*.

---

## License

MIT License - see LICENSE file for details

---

## Citation

If you use MBCAS in your research, please cite:

```bibtex
@software{mbcas2025,
  title = {MBCAS: Market-Based CPU Allocation System for Kubernetes},
  author = {Alaoui Sosse, Saad and Bouazza, Chaymae and Benabbou, Imane and Taqi, Mohamed Chadi},
  year = {2025},
  url = {https://github.com/ENSIAS-3A-Projects/Projet-Federateur}
}
```

---

## Contact

- **Institution**: ENSIAS, Mohammed V University
- **Email**: [chaditaqi2@gmail.com]
- **GitHub**: [https://github.com/ENSIAS-3A-Projects/Projet-Federateur]

---

**MBCAS**: Where game theory meets container orchestration ðŸŽ®ðŸ³